{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2bq5cxO0gX1",
        "outputId": "0729f2a9-24ef-4d08-8cc4-daf63f9927c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.1)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (6.26.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.2.0)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.5.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.5.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (23.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.11.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install gymnasium\n",
        "%pip install gymnasium[atari]\n",
        "%pip install gymnasium[accept-rom-license]\n",
        "%pip install --upgrade ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srn9-0mp0ejt",
        "outputId": "b8b2da57-c153-4a9d-832d-de117e7e1cda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-15 14:17:26.497410: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 14:17:26.497463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 14:17:26.497496: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 14:17:26.507018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n",
            "2023-11-15 14:17:28.202753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7aa932d3a020>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import get_backend\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FA_m8g1A0ejy"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'final_state_bool'))\n",
        "ramDict = dict(player_y=51, player_x=46, enemy_y=50, enemy_x=45, ball_x=49, ball_y=54) # Retrieved from github.com/mila-iqia/atari-representation-learning\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def create_model(num_actions):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(300, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_actions)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def map_action(action):\n",
        "    action_map = {0:0, 1:4, 2:5}\n",
        "    return action_map[action]\n",
        "\n",
        "def moving_average(a, n=3):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "def process_state(state):\n",
        "    state = state.reshape(1, -1)\n",
        "    state = state/255\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifbgYGni0ejz",
        "outputId": "f6104b9f-375a-437a-f89a-11089dde666d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "[Powered by Stella]\n",
            "2023-11-15 14:17:31.249080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:31.295699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:31.296055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:31.298599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:31.298920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:31.299162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:32.406407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:32.406726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:32.406859: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-11-15 14:17:32.406933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 14:17:32.407085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"ALE/Pong-ram-v5\") # Since we aren't using a convolution layer, we can use the ram version of the game\n",
        "state, info = env.reset()\n",
        "state = process_state(state)\n",
        "n_actions = 3 # Reduce the action space to only the relevant actions\n",
        "\n",
        "device = tf.device(\"/GPU:0\")\n",
        "weightDir = \"./q2a/policyWeights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oTT10eHG0ej0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.3\n",
        "EPS_DECAY = 50000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "loss_object = tf.keras.losses.Huber()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LR, clipvalue = 100)\n",
        "\n",
        "\n",
        "policy_net = create_model(n_actions)\n",
        "target_net = create_model(n_actions)\n",
        "\n",
        "policy_net(state)\n",
        "target_net(state)\n",
        "target_net.set_weights(policy_net.get_weights())\n",
        "\n",
        "memory = ReplayMemory(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SCW2rcGa0ej0"
      },
      "outputs": [],
      "source": [
        "def select_action(state, steps_done):\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * steps_done / EPS_DECAY)\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        return policy_net(state).numpy().argmax()\n",
        "    else:\n",
        "        return random.randrange(0, n_actions)\n",
        "\n",
        "def plot_state(state):\n",
        "\n",
        "    plt.clf()\n",
        "    plt.imshow(state)\n",
        "\n",
        "    if is_ipython:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WzOk6K1E0ej0"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation). This converts batch-array of Transitions to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements (a final state would've been the one after which simulation ended)\n",
        "    final_mask = tf.convert_to_tensor(batch.final_state_bool)\n",
        "    non_final_next_states = tf.reshape(tf.convert_to_tensor([s.next_state for s in transitions if not s.final_state_bool]), [sum(~final_mask.numpy()), -1])\n",
        "\n",
        "    state_batch = tf.reshape(tf.convert_to_tensor(batch.state), [BATCH_SIZE, -1])\n",
        "    action_batch = tf.reshape(tf.convert_to_tensor(batch.action), [BATCH_SIZE, -1])\n",
        "    reward_batch = tf.reshape(tf.convert_to_tensor(batch.reward), [BATCH_SIZE, -1])\n",
        "    reward_batch = (reward_batch - tf.reduce_mean(reward_batch)) / tf.math.reduce_std(reward_batch)\n",
        "\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "      tape.watch(policy_net.trainable_variables)\n",
        "\n",
        "      # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken. These are the actions which would've been taken for each batch state according to policy_net\n",
        "      state_action_values = tf.gather(policy_net(state_batch), action_batch, axis = 1, batch_dims = 1)\n",
        "\n",
        "      # Compute V(s_{t+1}) for all next states. Expected values of actions for non_final_next_states are computed based on the \"older\" target_net; selecting their best reward with max(1)[0]. This is merged based on the mask, such that we'll have either the expected state value or 0 in case the state was final.\n",
        "      next_state_values = np.zeros([BATCH_SIZE, 1])\n",
        "      next_state_values[~final_mask] = tf.reshape(tf.reduce_max(target_net(non_final_next_states), axis = 1), [sum(~final_mask.numpy()), 1])\n",
        "      # Compute the expected Q values\n",
        "      expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "      # Compute Huber loss\n",
        "      loss = loss_object(state_action_values, expected_state_action_values)\n",
        "\n",
        "\n",
        "    # Optimize the model\n",
        "    gradients = tape.gradient(loss, policy_net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, policy_net.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGdOR4yH0ej1",
        "outputId": "dff717d8-960a-4fb5-901d-ef7f98cb8b6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/500 [00:08<1:08:58,  8.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode:   0 \t return: -18.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 101/500 [09:43<37:27,  5.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 100 \t return: -20.366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 201/500 [19:39<30:11,  6.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 200 \t return: -20.478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 301/500 [29:33<19:18,  5.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 300 \t return: -20.512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 401/500 [39:29<10:10,  6.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 400 \t return: -20.506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [49:16<00:00,  5.91s/it]\n"
          ]
        }
      ],
      "source": [
        "num_episodes = 500\n",
        "sumr = 0\n",
        "steps_done = 0\n",
        "rewards = []\n",
        "\n",
        "for i_episode in tqdm(range(0, num_episodes)):\n",
        "    # Initialize the environment and get its state\n",
        "    state, info = env.reset(seed = i_episode)\n",
        "    state = process_state(state)\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = select_action(state, steps_done)\n",
        "        steps_done += 1\n",
        "        exec_action = map_action(action)\n",
        "        next_state, reward, terminated, truncated, info = env.step(exec_action)\n",
        "        next_state = process_state(next_state)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        sumr += reward\n",
        "        reward -= abs(next_state[0][ramDict[\"ball_y\"]] - next_state[0][ramDict[\"player_y\"]]) # Punishment for not moving towards the ball\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        new_weights = [TAU*x + (1-TAU)*y for x,y in zip(policy_net.get_weights(), target_net.get_weights())]\n",
        "        target_net.set_weights(new_weights)\n",
        "\n",
        "        if done:\n",
        "            if (i_episode + 1) % 50 == 0:\n",
        "                policy_net.save_weights(weightDir + \"_\" + str(i_episode+1))\n",
        "            rewards.append(sumr)\n",
        "            if (i_episode % 100) == 0:\n",
        "              print('episode: %3d \\t return: %.3f' % (i_episode, np.mean(rewards)))\n",
        "            sumr = 0\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv3oSyTV0ej2",
        "outputId": "e750ed41-562e-4327-fa4b-6386a80b7261"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:25<00:00,  2.55s/it]\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "rewards = []\n",
        "num_episodes = 10\n",
        "sumr = 0\n",
        "env = gym.make(\"ALE/Pong-ram-v5\") # Since we aren't using a convolution layer, we can use the ram version of the game\n",
        "state, info = env.reset()\n",
        "\n",
        "for i_episode in tqdm(range(num_episodes)):\n",
        "    # Initialize the environment and get its state\n",
        "    state, info = env.reset(seed=i_episode)\n",
        "    state = process_state(state)\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy_net(state).numpy().argmax()\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        next_state = process_state(next_state)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        sumr += reward\n",
        "\n",
        "        if done:\n",
        "            rewards.append(sumr)\n",
        "            sumr = 0\n",
        "            break\n",
        "\n",
        "plt.plot(rewards)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
